---
lab:
  title: Verwenden von Deltatabellen in Apache Spark
  module: Work with Delta Lake tables in Microsoft Fabric
---

# Verwenden von Deltatabellen in Apache Spark

Die Tabellen in einem Microsoft Fabric Lakehouse basieren auf dem Open-Source-Format Delta Lake. Delta Lake bietet Support f√ºr relationale Semantik sowohl f√ºr Batch- als auch f√ºr Streaming-Daten. In dieser √úbung werden Sie Deltatabellen erstellen und die Daten mithilfe von SQL-Abfragen untersuchen.

Diese √úbung dauert ca. **45**¬†Minuten.

> [!NOTE]
> Sie ben√∂tigen eine [Microsoft Fabric](/fabric/get-started/fabric-trial) Testversion, um diese √úbung durchzuf√ºhren.

## Erstellen eines Arbeitsbereichs

Erstellen Sie zun√§chst einen Arbeitsbereich mit  *Fabric* aktiviert.

1. Navigieren Sie in einem Browser unter `https://app.fabric.microsoft.com/home?experience=fabric` zur [Microsoft Fabric-Startseite](https://app.fabric.microsoft.com/home?experience=fabric)¬†und melden Sie sich mit Ihren Fabric-Anmeldeinformationen an.
1. W√§hlen Sie in der Men√ºleiste auf der linken Seite **Arbeitsbereiche** (üóá).
1. Erstellen Sie einen **neuen Arbeitsbereich** mit einem Namen Ihrer Wahl und w√§hlen Sie einen Lizenzierungsmodus, der Fabric-Kapazit√§t beinhaltet (Trial, Premium oder Fabric).
1. Wenn Ihr neuer Arbeitsbereich ge√∂ffnet wird, sollte er leer sein.

    ![Anzeigebild eines leeren Fabric-Arbeitsbereichs.](Images/workspace-empty.jpg)

## Erstellen eines Lakehouse und Hochladen von Daten

Jetzt, wo Sie einen Arbeitsbereich haben, ist es an der Zeit, ein Lakehouse zu erstellen und einige Daten hochzuladen.

1. W√§hlen Sie in der Men√ºleiste auf der linken Seite **Erstellen** aus. W√§hlen Sie auf der Seite *Neu* unter dem Abschnitt *Datentechnik* die Option **Lakehouse** aus. W√§hlen Sie einen eindeutigen Namen Ihrer Wahl aus.

    >**Hinweis**: Wenn die Option **Erstellen** nicht an die Seitenleiste angeheftet ist, m√ºssen Sie zuerst die Ellipses-Option (**‚Ä¶**) ausw√§hlen.

1. Es gibt verschiedene M√∂glichkeiten, Daten zu erfassen. In dieser √úbung laden Sie jedoch eine Textdatei auf Ihren lokalen Computer (oder ggf. auf die Lab-VM) herunter und laden sie dann auf Ihr Lakehouse hoch. Laden Sie die [Datendatei](https://github.com/MicrosoftLearning/dp-data/raw/main/products.csv) von `https://github.com/MicrosoftLearning/dp-data/raw/main/products.csv` herunter und speichern Sie sie als *Produkte.csv*.
1.  Kehren Sie zu der Registerkarte des Webbrowsers zur√ºck, die Ihr Lakehouse enth√§lt, und w√§hlen Sie im Explorer-Bereich neben dem Ordner **Dateien** das Men√º mit den drei Punkten (...) aus. Men√º.  Erstellen Sie einen **neuen Unterordner** namens *Produkte*.
1.  Geben Sie Feld Men√º f√ºr den Produktordner, **laden Sie die *products.csv* Datei von Ihrem lokalen Computer (oder lab VM, falls zutreffend) hoch**.
1.  Nachdem die Datei hochgeladen wurde, w√§hlen Sie den Ordner **Produkte** aus, um zu √ºberpr√ºfen, ob die Datei hochgeladen wurde, wie hier gezeigt:

    ![Anzeigebild von products.csv, das auf das Lakehouse hochgeladen wurde.](Images/upload-products.jpg)
  
## Untersuchen von Daten in einem Dataframe

1.  Erstellen Sie ein **neues Notebook**. Nach einigen Sekunden wird ein neues Notebook mit einer einzelnen Zelle ge√∂ffnet. Notebooks bestehen aus einer oder mehreren Zellen, die Code oder Markdown (formatierten Text) enthalten k√∂nnen.
2.  Markieren Sie die erste Zelle (die momentan eine Codezelle ist), und verwenden Sie dann in der oberen rechten Symbolleiste die Schaltfl√§che **M‚Üì**, um sie in eine Abschriftenzelle umzuwandeln. Der in der Zelle enthaltene Text wird dann als formatierter Text angezeigt. Verwenden Sie Markdownzellen, um erl√§uternde Informationen zu Ihrem Code bereitzustellen.
3.  Verwenden Sie die Schaltfl√§che üñâ (Bearbeiten), um die Zelle in den Bearbeitungsmodus zu schalten, und √§ndern Sie dann das Markdown wie folgt:

    ```markdown
    # Delta Lake tables 
    Use this notebook to explore Delta Lake functionality 
    ```

4. Klicken Sie au√üerhalb der Zelle auf eine beliebige Stelle im Notebook, um die Bearbeitung zu beenden und die gerenderte Markdownzelle anzuzeigen.
5. F√ºgen Sie eine neue Codezelle hinzu, und f√ºgen Sie den folgenden Code ein, um die Produktdaten mit Hilfe eines definierten Schemas in einen DataFrame zu lesen:

    ```python
    from pyspark.sql.types import StructType, IntegerType, StringType, DoubleType

    # define the schema
    schema = StructType() \
    .add("ProductID", IntegerType(), True) \
    .add("ProductName", StringType(), True) \
    .add("Category", StringType(), True) \
    .add("ListPrice", DoubleType(), True)

    df = spark.read.format("csv").option("header","true").schema(schema).load("Files/products/products.csv")
    # df now is a Spark DataFrame containing CSV data from "Files/products/products.csv".
    display(df)
    ```

> [!TIP]
> Blenden Sie die Explorer-Fenster aus oder ein, indem Sie das Chevron-Symbol ¬´ verwenden. So k√∂nnen Sie sich entweder auf das Notebook oder auf Ihre Dateien konzentrieren.

7. Benutzen Sie die Schaltfl√§che **Zelle ausf√ºhren** (‚ñ∑) auf der linken Seite der Zelle, um sie zu starten.

> [!NOTE]
> Da dies das erste Mal ist, dass Sie den Code in diesem Notebook ausf√ºhren, muss eine Spark-Sitzung gestartet werden. Dadurch kann der Abschluss der ersten Ausf√ºhrung etwa eine Minute dauern. Nachfolgende Ausf√ºhrungen erfolgen schneller.

8. Wenn der Zellenbefehl abgeschlossen ist, √ºberpr√ºfen Sie die Ausgabe unterhalb der Zelle, die wie folgt aussehen sollte:

    ![Anzeigebild der Daten aus products.csv.](Images/products-schema.jpg)
 
## Erstellen von Deltatabellen

Sie k√∂nnen den DataFrame als Delta-Tabelle speichern, indem Sie die Methode *saveAsTable* verwenden. Delta Lake unterst√ºtzt die Erstellung von verwalteten und externen Tabellen.

   * **Verwaltete** Delta-Tabellen profitieren von einer h√∂heren Leistung, da Fabric sowohl die Schema-Metadaten als auch die Datendateien verwaltet.
   * **Externe** Tabellen erm√∂glichen es Ihnen, Daten extern zu speichern, wobei die Metadaten von Fabric verwaltet werden.

### Erstellen einer verwalteten Tabelle

Die Datendateien werden im Ordner **Tabellen** erstellt.

1. Verwenden Sie unter den Ergebnissen der ersten Codezelle das Symbol ‚Äû+ Code‚Äú, um eine neue Codezelle hinzuzuf√ºgen.

> [!TIP]
> Um das Symbol f√ºr ‚Äû+ Code‚Äú zu sehen, bewegen Sie die Maus direkt unter und links neben die Ausgabe der aktuellen Zelle. Alternativ dazu k√∂nnen Sie in der Men√ºleiste auf der Registerkarte ‚ÄûBearbeiten‚Äú **+ Codezelle hinzuf√ºgen** ausw√§hlen.

2. Um eine verwaltete Deltatabelle zu erstellen, f√ºgen Sie eine neue Zelle hinzu, geben den folgenden Code ein und f√ºhren die Zelle dann aus:

    ```python
    df.write.format("delta").saveAsTable("managed_products")
    ```

3.  **Aktualisieren** Sie im Explorer von Lakehouse den Ordner ‚ÄûTabellen‚Äú und erweitern Sie den Tabellen-Knoten, um zu √ºberpr√ºfen, ob die Tabelle **managed_products** erstellt wurde.

>[!NOTE]
> Das Dreieckssymbol neben dem Dateinamen gibt eine Deltatabelle an.

Die Dateien f√ºr verwaltete Tabellen werden im Ordner **Tabellen** im Lakehouse gespeichert. Es wurde ein Ordner mit dem Namen *managed_products* erstellt, in dem die Parquet-Dateien und der Ordner ‚Äûdelta_log‚Äú f√ºr die Tabelle gespeichert werden.

### Erstellen einer externen Tabelle

Sie k√∂nnen auch externe Tabellen erstellen, die an einem anderen Ort als dem Lakehouse gespeichert sein k√∂nnen, wobei die Schemametadaten im Lakehouse gespeichert werden.

1.  Im Explorer-Fenster von Lakehouse, im Bereich ... Men√º f√ºr den Ordner **Dateien**, w√§hlen Sie **ABFS-Pfad kopieren**. Der ABFS-Pfad ist der vollst√§ndig qualifizierte Pfad zum Ordner mit den Lakehouse-Dateien.

2.  F√ºgen Sie den ABFS-Pfad in eine neue Codezelle ein. F√ºgen Sie den folgenden Code hinzu und verwenden Sie Ausschneiden und Einf√ºgen, um den abfs_path an der richtigen Stelle im Code einzuf√ºgen:

    ```python
    df.write.format("delta").saveAsTable("external_products", path="abfs_path/external_products")
    ```

3. Der vollst√§ndige Pfad sollte etwa wie folgt aussehen:

    ```python
    abfss://workspace@tenant-onelake.dfs.fabric.microsoft.com/lakehousename.Lakehouse/Files/external_products
    ```

4. **F√ºhren Sie** die Zelle aus, um den DataFrame als externe Tabelle im Ordner ‚ÄûFiles/external_products‚Äú zu speichern.

5.  **Aktualisieren** Sie im Lakehouse-Explorerfenster den Tabellenordner und erweitern Sie den Tabellenknoten. √úberpr√ºfen Sie, ob die Tabelle ‚Äûexternal_products‚Äú mit den Schemametadaten erstellt wurde.

6.  Im Explorer-Fenster von Lakehouse, im Bereich ... Men√º f√ºr den Ordner ‚ÄûDateien‚Äú, w√§hlen Sie **Aktualisieren**. Erweitern Sie dann den Knoten ‚ÄûDateien‚Äú und √ºberpr√ºfen Sie, ob der Ordner ‚Äûexternal_products‚Äú f√ºr die Datendateien der Tabelle erstellt wurde.

### Vergleichen verwalteter und externer Tabellen

Lassen Sie uns die Unterschiede zwischen verwalteten und externen Tabellen mithilfe des Magic-Befehls ‚Äû%%sql‚Äú erkunden.

1. Geben Sie in eine neue Codezelle den folgenden Code ein:

    ```python
    %%sql
    DESCRIBE FORMATTED managed_products;
    ```

2. In den Ergebnissen k√∂nnen Sie die Tabelle nach der Speicherorteigenschaft anzeigen. Klicken Sie auf den Speicherortwert in der Spalte ‚ÄûDatentyp‚Äú, um den vollst√§ndigen Pfad anzuzeigen. Beachten Sie, dass der OneLake-Speicherort mit /Tables/managed_products endet.

3. √Ñndern Sie den DESCRIBE-Befehl, um die Details der external_products-Tabelle wie hier dargestellt anzuzeigen:

    ```python
    %%sql
    DESCRIBE FORMATTED external_products;
    ```

4. F√ºhren Sie die Zelle aus und sehen Sie sich in den Ergebnissen die Eigenschaft ‚ÄûSpeicherort‚Äú f√ºr die Tabelle an. Erweitern Sie die Spalte ‚ÄûDatentyp‚Äú, um den vollst√§ndigen Pfad anzuzeigen, und beachten Sie, dass die OneLake-Speicherorte mit /Files/external_products enden.

5. Geben Sie in eine neue Codezelle den folgenden Code ein:

    ```python
    %%sql
    DROP TABLE managed_products;
    DROP TABLE external_products;
    ```

6. **Aktualisieren** Sie im Lakehouse-Explorerfenster den Tabellenordner, um zu √ºberpr√ºfen, dass im Tabellenknoten keine Tabellen aufgef√ºhrt sind.
7.  **Aktualisieren** Sie im Lakehouse- Explorerfenster den Ordner ‚ÄûDateien‚Äú und stellen Sie sicher, dass die Datei ‚Äûexternal_products‚Äú *nicht* gel√∂scht wurde. W√§hlen Sie diesen Ordner, um die Parquet-Datendateien und den Ordner ‚Äû_delta_log‚Äú anzuzeigen. 

Die Metadaten f√ºr die externe Tabelle wurden gel√∂scht, aber nicht die Datendatei.

## Verwenden Sie SQL, um eine Deltatabelle zu erstellen

Sie erstellen nun eine Deltatabelle mit dem Magic-Befehl ‚Äû%%sql‚Äú. 

1. F√ºgen Sie eine weitere Codezelle hinzu, und f√ºhren Sie den Code aus:

    ```python
    %%sql
    CREATE TABLE products
    USING DELTA
    LOCATION 'Files/external_products';
    ```

2. Im Explorer-Fenster von Lakehouse, im Bereich ... Men√º f√ºr den Ordner **Tabellen**, w√§hlen Sie **Aktualisieren**. Erweitern Sie dann den Tabellenknoten und √ºberpr√ºfen Sie, ob eine neue Tabelle mit dem Namen *Produkte* aufgef√ºhrt ist. Erweitern Sie dann die Tabelle, um das Schema anzuzeigen.

3. F√ºgen Sie eine weitere Codezelle hinzu, und f√ºhren Sie den Code aus:

    ```python
    %%sql
    SELECT * FROM products;
    ```

## Erkunden der Tabellenversionsverwaltung

Der Transaktionsverlauf f√ºr Deltatabellen wird in JSON-Dateien im Ordner ‚Äûdelta_log‚Äú gespeichert. Sie k√∂nnen dieses Transaktionsprotokoll verwenden, um die Datenversionsverwaltung zu verwalten.

1.  F√ºgen Sie dem Notebook eine neue Codezelle hinzu, und f√ºhren Sie den folgenden Code aus, der eine Reduzierung des Preises von 10 % f√ºr Mountainbikes implementiert:

    ```python
    %%sql
    UPDATE products
    SET ListPrice = ListPrice * 0.9
    WHERE Category = 'Mountain Bikes';
    ```

2. F√ºgen Sie eine weitere Codezelle hinzu, und f√ºhren Sie den Code aus:

    ```python
    %%sql
    DESCRIBE HISTORY products;
    ```

Die Ergebnisse zeigen den Verlauf der Transaktionen, die f√ºr die Tabelle aufgezeichnet wurden.

3.  F√ºgen Sie eine weitere Codezelle hinzu, und f√ºhren Sie den Code aus:

    ```python
    delta_table_path = 'Files/external_products'
    # Get the current data
    current_data = spark.read.format("delta").load(delta_table_path)
    display(current_data)

    # Get the version 0 data
    original_data = spark.read.format("delta").option("versionAsOf", 0).load(delta_table_path)
    display(original_data)
    ```

Es werden zwei Ergebniss√§tze zur√ºckgegeben - einer mit den Daten nach der Preissenkung, der andere mit der urspr√ºnglichen Version der Daten.

## Analysieren von Daten in Deltatabellen mit SQL-Abfragen

Mit dem SQL Magic-Befehl k√∂nnen Sie die SQL-Syntax anstelle von Pyspark verwenden. Hier erstellen Sie eine tempor√§re Ansicht aus der Produkttabelle mit einer `SELECT`-Anweisung.

1. F√ºgen Sie eine neue Codezelle hinzu, und f√ºhren Sie den folgenden Code aus, um die tempor√§re Ansicht zu erstellen und anzuzeigen:

    ```python
    %%sql
    -- Create a temporary view
    CREATE OR REPLACE TEMPORARY VIEW products_view
    AS
        SELECT Category, COUNT(*) AS NumProducts, MIN(ListPrice) AS MinPrice, MAX(ListPrice) AS MaxPrice, AVG(ListPrice) AS AvgPrice
        FROM products
        GROUP BY Category;

    SELECT *
    FROM products_view
    ORDER BY Category;    
    ```

2. F√ºgen Sie eine neue Codezelle hinzu, und f√ºhren Sie den folgenden Code aus, um die 10 wichtigsten Kategorien nach Anzahl der Produkte zu ermitteln:

    ```python
    %%sql
    SELECT Category, NumProducts
    FROM products_view
    ORDER BY NumProducts DESC
    LIMIT 10;
    ```

3. Wenn die Daten zur√ºckgegeben werden, w√§hlen Sie die Ansicht **Diagramm**, um ein Balkendiagramm anzuzeigen.

    ![Anzeigebild der SQL-Anweisung Ausw√§hlen und der Ergebnisse.](Images/sql-select.jpg)

Alternativ k√∂nnen Sie eine SQL-Abfrage mit PySpark ausf√ºhren.

4. F√ºgen Sie den folgenden Code in einer neuen Codezelle hinzu und f√ºhren Sie ihn aus:

    ```python
    from pyspark.sql.functions import col, desc

    df_products = spark.sql("SELECT Category, MinPrice, MaxPrice, AvgPrice FROM products_view").orderBy(col("AvgPrice").desc())
    display(df_products.limit(6))
    ```

## Verwenden von Delta-Tabellen f√ºr Streaming-Daten

Delta Lake unterst√ºtzt Streaming-Daten. Deltatabellen k√∂nnen eine Senke oder Quelle f√ºr Datenstr√∂me sein, die mit der Spark Structured Streaming-API erstellt wurden. In diesem Beispiel verwenden Sie eine Deltatabelle als Senke f√ºr einige Streamingdaten in einem simulierten IoT-Szenario (Internet der Dinge).

1.  F√ºgen Sie eine neue Codezelle hinzu, f√ºgen Sie den folgenden Code ein und f√ºhren Sie ihn aus:

    ```python
    from notebookutils import mssparkutils
    from pyspark.sql.types import *
    from pyspark.sql.functions import *

    # Create a folder
    inputPath = 'Files/data/'
    mssparkutils.fs.mkdirs(inputPath)

    # Create a stream that reads data from the folder, using a JSON schema
    jsonSchema = StructType([
    StructField("device", StringType(), False),
    StructField("status", StringType(), False)
    ])
    iotstream = spark.readStream.schema(jsonSchema).option("maxFilesPerTrigger", 1).json(inputPath)

    # Write some event data to the folder
    device_data = '''{"device":"Dev1","status":"ok"}
    {"device":"Dev1","status":"ok"}
    {"device":"Dev1","status":"ok"}
    {"device":"Dev2","status":"error"}
    {"device":"Dev1","status":"ok"}
    {"device":"Dev1","status":"error"}
    {"device":"Dev2","status":"ok"}
    {"device":"Dev2","status":"error"}
    {"device":"Dev1","status":"ok"}'''

    mssparkutils.fs.put(inputPath + "data.txt", device_data, True)

    print("Source stream created...")
    ```

Sicherstellen, dass die Meldung *Quell-Stream erstellt...* angezeigt. Der Code, den Sie gerade ausgef√ºhrt haben, hat eine Streamingdatenquelle basierend auf einem Ordner erstellt, in dem einige Daten gespeichert wurden, die Messwerte von hypothetischen IoT-Ger√§ten darstellen.

2. F√ºgen Sie den folgenden Code in einer neuen Codezelle hinzu, und f√ºhren Sie ihn aus:

    ```python
    # Write the stream to a delta table
    delta_stream_table_path = 'Tables/iotdevicedata'
    checkpointpath = 'Files/delta/checkpoint'
    deltastream = iotstream.writeStream.format("delta").option("checkpointLocation", checkpointpath).start(delta_stream_table_path)
    print("Streaming to delta sink...")
    ```

Dieser Code schreibt die Streamingger√§tedaten im Deltaformat in einen Ordner mit dem Namen iotdevicedata. Da der Pfad f√ºr den Ordnerspeicherort im Ordner Tabellen angegeben ist, wird automatisch eine Tabelle f√ºr ihn erstellt.

3. F√ºgen Sie den folgenden Code in einer neuen Codezelle hinzu, und f√ºhren Sie ihn aus:

    ```python
    %%sql
    SELECT * FROM IotDeviceData;
    ```

Dieser Code fragt die Tabelle IotDeviceData ab, die die Ger√§tedaten aus der Streamingquelle enth√§lt.

4. F√ºgen Sie den folgenden Code in einer neuen Codezelle hinzu, und f√ºhren Sie ihn aus:

    ```python
    # Add more data to the source stream
    more_data = '''{"device":"Dev1","status":"ok"}
    {"device":"Dev1","status":"ok"}
    {"device":"Dev1","status":"ok"}
    {"device":"Dev1","status":"ok"}
    {"device":"Dev1","status":"error"}
    {"device":"Dev2","status":"error"}
    {"device":"Dev1","status":"ok"}'''

    mssparkutils.fs.put(inputPath + "more-data.txt", more_data, True)
    ```

Dieser Code schreibt weitere hypothetische Ger√§tedaten in die Streamingquelle.

5. F√ºhren Sie die Zelle mit dem folgenden Code erneut aus:

    ```python
    %%sql
    SELECT * FROM IotDeviceData;
    ```

Dieser Code fragt die IotDeviceData-Tabelle erneut ab, die nun die zus√§tzlichen Daten enthalten sollte, die der Streamingquelle hinzugef√ºgt wurden.

6. F√ºgen Sie in einer neuen Codezelle Code hinzu, um den Datenstrom zu beenden und die Zelle auszuf√ºhren:

    ```python
    deltastream.stop()
    ```

## Bereinigen von Ressourcen

In dieser √úbung haben Sie gelernt, wie man mit Deltatabellen in Microsoft Fabric arbeitet.

Wenn Sie mit der Erkundung Ihres Lakehouses fertig sind, k√∂nnen Sie den f√ºr diese √úbung erstellten Arbeitsbereich l√∂schen.

1. W√§hlen Sie auf der Leiste auf der linken Seite das Symbol f√ºr Ihren Arbeitsbereich aus, um alle darin enthaltenen Elemente anzuzeigen.
2. Geben Sie Feld Men√º auf der Symbolleiste, w√§hlen Sie **Arbeitsbereichseinstellungen**.
3. W√§hlen Sie im Abschnitt Allgemein die Option **Diesen Arbeitsbereich entfernen**.
