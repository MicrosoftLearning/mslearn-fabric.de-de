---
lab:
  title: Verwenden von Deltatabellen in Apache Spark
  module: Work with Delta Lake tables in Microsoft Fabric
---

# Verwenden von Deltatabellen in Apache Spark

Die Tabellen in einem Microsoft Fabric Lakehouse basieren auf dem Open-Source-Format Delta Lake. Delta Lake bietet Support f√ºr relationale Semantik sowohl f√ºr Batch- als auch f√ºr Streaming-Daten. In dieser √úbung werden Sie Deltatabellen erstellen und die Daten mithilfe von SQL-Abfragen untersuchen.

Diese √úbung dauert ca. **45**¬†Minuten.

> [!Note] 
> Sie ben√∂tigen Zugriff auf einen [Microsoft Fabric-Mandanten](https://learn.microsoft.com/fabric/get-started/fabric-trial), um diese √úbung abzuschlie√üen.

## Erstellen eines Arbeitsbereichs

Bevor Sie mit Daten in Fabric arbeiten, erstellen Sie einen Arbeitsbereich in einem Mandanten mit aktivierter Fabric-Kapazit√§t.

1. Navigieren Sie in einem Browser unter `https://app.fabric.microsoft.com/home?experience=fabric-developer` zur [Microsoft Fabric-Startseite](https://app.fabric.microsoft.com/home?experience=fabric-developer) und melden Sie sich mit Ihren Fabric-Anmeldeinformationen an.
1. W√§hlen Sie auf der Men√ºleiste auf der linken Seite **Arbeitsbereiche** aus (Symbol √§hnelt &#128455;).
1. Erstellen Sie einen neuen Arbeitsbereich mit einem Namen Ihrer Wahl, und w√§hlen Sie im Bereich **Erweitert** einen Lizenzierungsmodus mit Fabric-Kapazit√§ten aus (*Testversion*, *Premium* oder *Fabric*).
1. Wenn Ihr neuer Arbeitsbereich ge√∂ffnet wird, sollte er leer sein.

    ![Screenshot eines leeren Arbeitsbereichs in Fabric](./Images/new-workspace.png)

## Erstellen eines Lakehouse und Hochladen von Dateien

Jetzt, da Sie einen Arbeitsbereich haben, ist es an der Zeit, ein Data Lakehouse f√ºr Ihre Daten zu erstellen.

1. W√§hlen Sie in der Men√ºleiste auf der linken Seite **Erstellen** aus. W√§hlen Sie auf der Seite *Neu* unter dem Abschnitt *Datentechnik* die Option **Lakehouse** aus. W√§hlen Sie einen eindeutigen Namen Ihrer Wahl aus.

    >**Hinweis**: Wenn die Option **Erstellen** nicht an die Seitenleiste angeheftet ist, m√ºssen Sie zuerst die Ellipses-Option (**‚Ä¶**) ausw√§hlen.

    Nach etwa einer Minute wird ein neues Lakehouse erstellt:

    ![Screenshot: Neues Lakehouse](./Images/new-lakehouse.png)

1. Sehen Sie sich das neue Lakehouse an, und beachten Sie, dass Sie im Bereich **Explorer** auf der linken Seite Tabellen und Dateien im Lakehouse durchsuchen k√∂nnen:

Sie k√∂nnen nun Daten in das Lakehouse aufnehmen. Es gibt mehrere M√∂glichkeiten, dies zu tun. F√ºr den Moment laden Sie jedoch eine Textdatei auf Ihren lokalen Computer (oder gegebenenfalls auf die VM im Lab) herunter und laden sie anschlie√üend in Ihr Lakehouse hoch. 

1. Laden Sie die [Datendatei](https://github.com/MicrosoftLearning/dp-data/raw/main/products.csv) von `https://github.com/MicrosoftLearning/dp-data/raw/main/products.csv` herunter und speichern Sie sie als *Produkte.csv*.
1. Kehren Sie zu der Registerkarte des Webbrowsers zur√ºck, die Ihr Lakehouse enth√§lt, und w√§hlen Sie im Explorer-Bereich neben dem Ordner **Dateien** das Men√º mit den drei Punkten (...) aus. Men√º.  Erstellen Sie einen **neuen Unterordner** namens *Produkte*.
1. Geben Sie Feld Men√º f√ºr den Produktordner, **laden Sie die *products.csv* Datei von Ihrem lokalen Computer (oder lab VM, falls zutreffend) hoch**.
1. Nachdem die Datei hochgeladen wurde, w√§hlen Sie den Ordner **Produkte** aus, um zu √ºberpr√ºfen, ob die Datei hochgeladen wurde, wie hier gezeigt:

    ![Anzeigebild von products.csv, das auf das Lakehouse hochgeladen wurde.](Images/upload-products.png)
  
## Untersuchen von Daten in einem Dataframe

Sie k√∂nnen nun ein Fabric-Notizbuch erstellen, um mit Ihren Daten zu arbeiten. Notebooks stellen eine interaktive Umgebung bereit, in der Sie Code schreiben und ausf√ºhren k√∂nnen.

1. W√§hlen Sie in der Men√ºleiste auf der linken Seite **Erstellen** aus. W√§hlen Sie auf der Seite *Neu* im Abschnitt *Datentechnik* die Option **Notebook** aus.

    Ein neues Notebook mit dem Namen **Notebook 1** wird erstellt und ge√∂ffnet.

    ![Screenshot eines neuen Notebooks](./Images/new-notebook.png)

1. Fabric weist jedem Notizbuch, das Sie erstellen, einen Namen zu, z. B. Notizbuch 1, Notizbuch 2 usw. Klicken Sie auf das Bedienfeld oberhalb der Registerkarte **Home** im Men√º, um den Namen in einen aussagekr√§ftigeren Namen zu √§ndern.
1. Markieren Sie die erste Zelle (die momentan eine Codezelle ist), und verwenden Sie dann in der oberen rechten Symbolleiste die Schaltfl√§che **M‚Üì**, um sie in eine Abschriftenzelle umzuwandeln. Der in der Zelle enthaltene Text wird dann als formatierter Text angezeigt.
1. Verwenden Sie die Taste üñâ (Bearbeiten), um die Zelle in den Bearbeitungsmodus zu schalten, und √§ndern Sie dann den Markdown wie unten gezeigt.

    ```markdown
    # Delta Lake tables 
    Use this notebook to explore Delta Lake functionality 
    ```

1. Klicken Sie an einer beliebigen Stelle im Notebook au√üerhalb der Zelle, um die Bearbeitung zu beenden.
1. W√§hlen Sie im **Explorer**-Bereich die Option **Datenelemente hinzuf√ºgen** und dann **Vorhandene Datenquellen** aus. Stellen Sie eine Verbindung mit dem zuvor erstellten Lakehouse her.
1. F√ºgen Sie eine neue Codezelle hinzu, und f√ºgen Sie den folgenden Code ein, um die Produktdaten mit Hilfe eines definierten Schemas in einen DataFrame zu lesen:

    ```python
   from pyspark.sql.types import StructType, IntegerType, StringType, DoubleType

   # define the schema
   schema = StructType() \
   .add("ProductID", IntegerType(), True) \
   .add("ProductName", StringType(), True) \
   .add("Category", StringType(), True) \
   .add("ListPrice", DoubleType(), True)

   df = spark.read.format("csv").option("header","true").schema(schema).load("Files/products/products.csv")
   # df now is a Spark DataFrame containing CSV data from "Files/products/products.csv".
   display(df)
    ```

> [!TIP]
> Blenden Sie die Explorer-Fenster aus oder ein, indem Sie das Chevron-Symbol ¬´ verwenden. So k√∂nnen Sie sich entweder auf das Notebook oder auf Ihre Dateien konzentrieren.

1. Benutzen Sie die Schaltfl√§che **Zelle ausf√ºhren** (‚ñ∑) auf der linken Seite der Zelle, um sie zu starten.

> [!NOTE]
> Da dies das erste Mal ist, dass Sie den Code in diesem Notebook ausf√ºhren, muss eine Spark-Sitzung gestartet werden. Dadurch kann der Abschluss der ersten Ausf√ºhrung etwa eine Minute dauern. Nachfolgende Ausf√ºhrungen erfolgen schneller.

1. Wenn der Zellenbefehl abgeschlossen ist, √ºberpr√ºfen Sie die Ausgabe unterhalb der Zelle, die wie folgt aussehen sollte:

    ![Anzeigebild der Daten aus products.csv.](Images/products-schema.png)
 
## Erstellen von Deltatabellen

Sie k√∂nnen den DataFrame als Delta-Tabelle speichern, indem Sie die Methode *saveAsTable* verwenden. Delta Lake unterst√ºtzt die Erstellung von verwalteten und externen Tabellen.

   * **Verwaltete** Delta-Tabellen profitieren von einer h√∂heren Leistung, da Fabric sowohl die Schema-Metadaten als auch die Datendateien verwaltet.
   * **Externe** Tabellen erm√∂glichen es Ihnen, Daten extern zu speichern, wobei die Metadaten von Fabric verwaltet werden.

### Erstellen einer verwalteten Tabelle

Die Datendateien werden im Ordner **Tabellen** erstellt.

1. Verwenden Sie unter den Ergebnissen der ersten Codezelle das Symbol ‚Äû+ Code‚Äú, um eine neue Codezelle hinzuzuf√ºgen.

> [!TIP]
> Um das Symbol f√ºr ‚Äû+ Code‚Äú zu sehen, bewegen Sie die Maus direkt unter und links neben die Ausgabe der aktuellen Zelle. Alternativ dazu k√∂nnen Sie in der Men√ºleiste auf der Registerkarte ‚ÄûBearbeiten‚Äú **+ Codezelle hinzuf√ºgen** ausw√§hlen.

1. Um eine verwaltete Deltatabelle zu erstellen, f√ºgen Sie eine neue Zelle hinzu, geben den folgenden Code ein und f√ºhren die Zelle dann aus:

    ```python
   df.write.format("delta").saveAsTable("managed_products")
    ```

1. **Aktualisieren** Sie im Explorer den Ordner ‚ÄûTabellen‚Äú und erweitern Sie den Tabellen-Knoten, um zu √ºberpr√ºfen, ob die Tabelle **managed_products** erstellt wurde.

> [!NOTE]
> Das Dreieckssymbol neben dem Dateinamen gibt eine Deltatabelle an.

Die Dateien f√ºr verwaltete Tabellen werden im Ordner **Tabellen** im Lakehouse gespeichert. Es wurde ein Ordner mit dem Namen *managed_products* erstellt, in dem die Parquet-Dateien und der Ordner ‚Äûdelta_log‚Äú f√ºr die Tabelle gespeichert werden.

### Erstellen einer externen Tabelle

Sie k√∂nnen auch externe Tabellen erstellen, die an einem anderen Ort als dem Lakehouse gespeichert sein k√∂nnen, wobei die Schemametadaten im Lakehouse gespeichert werden.

1. Im Explorer-Fenster, im ... Men√º f√ºr den Ordner **Dateien**, w√§hlen Sie **ABFS-Pfad kopieren**. Der ABFS-Pfad ist der vollst√§ndig qualifizierte Pfad zum Ordner mit den Lakehouse-Dateien.

1. F√ºgen Sie den ABFS-Pfad in eine neue Codezelle ein. F√ºgen Sie den folgenden Code hinzu und verwenden Sie Ausschneiden und Einf√ºgen, um den abfs_path an der richtigen Stelle im Code einzuf√ºgen:

    ```python
   df.write.format("delta").saveAsTable("external_products", path="abfs_path/external_products")
    ```

1. Der vollst√§ndige Pfad sollte etwa wie folgt aussehen:

    ```python
   abfss://workspace@tenant-onelake.dfs.fabric.microsoft.com/lakehousename.Lakehouse/Files/external_products
    ```

1. **F√ºhren Sie** die Zelle aus, um den DataFrame als externe Tabelle im Ordner ‚ÄûFiles/external_products‚Äú zu speichern.

1. **Aktualisieren** Sie im Explorerfenster den Tabellenordner und erweitern Sie den Tabellenknoten. √úberpr√ºfen Sie, ob die Tabelle ‚Äûexternal_products‚Äú mit den Schemametadaten erstellt wurde.

1. Im Explorer-Fenster, im ... Men√º f√ºr den Ordner ‚ÄûDateien‚Äú, w√§hlen Sie **Aktualisieren**. Erweitern Sie dann den Knoten ‚ÄûDateien‚Äú und √ºberpr√ºfen Sie, ob der Ordner ‚Äûexternal_products‚Äú f√ºr die Datendateien der Tabelle erstellt wurde.

### Vergleichen verwalteter und externer Tabellen

Lassen Sie uns die Unterschiede zwischen verwalteten und externen Tabellen mithilfe des Magic-Befehls ‚Äû%%sql‚Äú erkunden.

1. Geben Sie in eine neue Codezelle den folgenden Code ein:

    ```python
   %%sql
   DESCRIBE FORMATTED managed_products;
    ```

1. In den Ergebnissen k√∂nnen Sie die Tabelle nach der Speicherorteigenschaft anzeigen. Klicken Sie auf den Speicherortwert in der Spalte ‚ÄûDatentyp‚Äú, um den vollst√§ndigen Pfad anzuzeigen. Beachten Sie, dass der OneLake-Speicherort mit /Tables/managed_products endet.

1. √Ñndern Sie den DESCRIBE-Befehl, um die Details der external_products-Tabelle wie hier dargestellt anzuzeigen:

    ```python
   %%sql
   DESCRIBE FORMATTED external_products;
    ```

1. F√ºhren Sie die Zelle aus und sehen Sie sich in den Ergebnissen die Eigenschaft ‚ÄûSpeicherort‚Äú f√ºr die Tabelle an. Erweitern Sie die Spalte ‚ÄûDatentyp‚Äú, um den vollst√§ndigen Pfad anzuzeigen, und beachten Sie, dass die OneLake-Speicherorte mit /Files/external_products enden.

1. Geben Sie in eine neue Codezelle den folgenden Code ein:

    ```python
   %%sql
   DROP TABLE managed_products;
   DROP TABLE external_products;
    ```

1. **Aktualisieren** Sie im Explorerfenster den Tabellenordner, um zu √ºberpr√ºfen, dass im Tabellenknoten keine Tabellen aufgef√ºhrt sind.
1. **Aktualisieren** Sie im Explorerfenster den Ordner ‚ÄûDateien‚Äú und stellen Sie sicher, dass die Datei ‚Äûexternal_products‚Äú *nicht* gel√∂scht wurde. W√§hlen Sie diesen Ordner, um die Parquet-Datendateien und den Ordner ‚Äû_delta_log‚Äú anzuzeigen. 

Die Metadaten f√ºr die externe Tabelle wurden gel√∂scht, aber nicht die Datendatei.

## Verwenden Sie SQL, um eine Deltatabelle zu erstellen

Sie erstellen nun eine Deltatabelle mit dem Magic-Befehl ‚Äû%%sql‚Äú. 

1. F√ºgen Sie eine weitere Codezelle hinzu, und f√ºhren Sie den Code aus:

    ```python
   %%sql
   CREATE TABLE products
   USING DELTA
   LOCATION 'Files/external_products';
    ```

1. Im Explorer-Fenster, im ... Men√º f√ºr den Ordner **Tabellen**, w√§hlen Sie **Aktualisieren**. Erweitern Sie dann den Tabellenknoten und √ºberpr√ºfen Sie, ob eine neue Tabelle mit dem Namen *Produkte* aufgef√ºhrt ist. Erweitern Sie dann die Tabelle, um das Schema anzuzeigen.
1. F√ºgen Sie eine weitere Codezelle hinzu, und f√ºhren Sie den Code aus:

    ```python
   %%sql
   SELECT * FROM products;
    ```

## Erkunden der Tabellenversionsverwaltung

Der Transaktionsverlauf f√ºr Deltatabellen wird in JSON-Dateien im Ordner ‚Äûdelta_log‚Äú gespeichert. Sie k√∂nnen dieses Transaktionsprotokoll verwenden, um die Datenversionsverwaltung zu verwalten.

1. F√ºgen Sie dem Notebook eine neue Codezelle hinzu, und f√ºhren Sie den folgenden Code aus, der eine Reduzierung des Preises von 10 % f√ºr Mountainbikes implementiert:

    ```python
   %%sql
   UPDATE products
   SET ListPrice = ListPrice * 0.9
   WHERE Category = 'Mountain Bikes';
    ```

1. F√ºgen Sie eine weitere Codezelle hinzu, und f√ºhren Sie den Code aus:

    ```python
   %%sql
   DESCRIBE HISTORY products;
    ```

Die Ergebnisse zeigen den Verlauf der Transaktionen, die f√ºr die Tabelle aufgezeichnet wurden.

1. F√ºgen Sie eine weitere Codezelle hinzu, und f√ºhren Sie den Code aus:

    ```python
   delta_table_path = 'Files/external_products'
   # Get the current data
   current_data = spark.read.format("delta").load(delta_table_path)
   display(current_data)

   # Get the version 0 data
   original_data = spark.read.format("delta").option("versionAsOf", 0).load(delta_table_path)
   display(original_data)
    ```

Es werden zwei Ergebniss√§tze zur√ºckgegeben - einer mit den Daten nach der Preissenkung, der andere mit der urspr√ºnglichen Version der Daten.

## Analysieren von Daten in Deltatabellen mit SQL-Abfragen

Mit dem SQL Magic-Befehl k√∂nnen Sie die SQL-Syntax anstelle von Pyspark verwenden. Hier erstellen Sie eine tempor√§re Ansicht aus der Produkttabelle mit einer `SELECT`-Anweisung.

1. F√ºgen Sie eine neue Codezelle hinzu, und f√ºhren Sie den folgenden Code aus, um die tempor√§re Ansicht zu erstellen und anzuzeigen:

    ```python
   %%sql
   -- Create a temporary view
   CREATE OR REPLACE TEMPORARY VIEW products_view
   AS
       SELECT Category, COUNT(*) AS NumProducts, MIN(ListPrice) AS MinPrice, MAX(ListPrice) AS MaxPrice, AVG(ListPrice) AS AvgPrice
       FROM products
       GROUP BY Category;

   SELECT *
   FROM products_view
   ORDER BY Category;    
    ```

1. F√ºgen Sie eine neue Codezelle hinzu, und f√ºhren Sie den folgenden Code aus, um die 10 wichtigsten Kategorien nach Anzahl der Produkte zu ermitteln:

    ```python
   %%sql
   SELECT Category, NumProducts
   FROM products_view
   ORDER BY NumProducts DESC
   LIMIT 10;
    ```

1. Wenn die Daten zur√ºckgegeben werden, w√§hlen Sie **+Neues Diagramm** aus, um eines der vorgeschlagenen Diagramme anzuzeigen.

    ![Anzeigebild der SQL-Anweisung Ausw√§hlen und der Ergebnisse.](Images/sql-select.png)

Alternativ k√∂nnen Sie eine SQL-Abfrage mit PySpark ausf√ºhren.

1. F√ºgen Sie den folgenden Code in einer neuen Codezelle hinzu und f√ºhren Sie ihn aus:

    ```python
   from pyspark.sql.functions import col, desc

   df_products = spark.sql("SELECT Category, MinPrice, MaxPrice, AvgPrice FROM products_view").orderBy(col("AvgPrice").desc())
   display(df_products.limit(6))
    ```

## Verwenden von Delta-Tabellen f√ºr Streaming-Daten

Delta Lake unterst√ºtzt Streaming-Daten. Deltatabellen k√∂nnen eine Senke oder Quelle f√ºr Datenstr√∂me sein, die mit der Spark Structured Streaming-API erstellt wurden. In diesem Beispiel verwenden Sie eine Deltatabelle als Senke f√ºr einige Streamingdaten in einem simulierten IoT-Szenario (Internet der Dinge).

1.  F√ºgen Sie eine neue Codezelle hinzu, f√ºgen Sie den folgenden Code ein und f√ºhren Sie ihn aus:

    ```python
    from notebookutils import mssparkutils
    from pyspark.sql.types import *
    from pyspark.sql.functions import *

    # Create a folder
    inputPath = 'Files/data/'
    mssparkutils.fs.mkdirs(inputPath)

    # Create a stream that reads data from the folder, using a JSON schema
    jsonSchema = StructType([
    StructField("device", StringType(), False),
    StructField("status", StringType(), False)
    ])
    iotstream = spark.readStream.schema(jsonSchema).option("maxFilesPerTrigger", 1).json(inputPath)

    # Write some event data to the folder
    device_data = '''{"device":"Dev1","status":"ok"}
    {"device":"Dev1","status":"ok"}
    {"device":"Dev1","status":"ok"}
    {"device":"Dev2","status":"error"}
    {"device":"Dev1","status":"ok"}
    {"device":"Dev1","status":"error"}
    {"device":"Dev2","status":"ok"}
    {"device":"Dev2","status":"error"}
    {"device":"Dev1","status":"ok"}'''

    mssparkutils.fs.put(inputPath + "data.txt", device_data, True)

    print("Source stream created...")
    ```

Sicherstellen, dass die Meldung *Quell-Stream erstellt...* angezeigt. Der Code, den Sie gerade ausgef√ºhrt haben, hat eine Streamingdatenquelle basierend auf einem Ordner erstellt, in dem einige Daten gespeichert wurden, die Messwerte von hypothetischen IoT-Ger√§ten darstellen.

1. F√ºgen Sie den folgenden Code in einer neuen Codezelle hinzu, und f√ºhren Sie ihn aus:

    ```python
   # Write the stream to a delta table
   delta_stream_table_path = 'Tables/iotdevicedata'
   checkpointpath = 'Files/delta/checkpoint'
   deltastream = iotstream.writeStream.format("delta").option("checkpointLocation", checkpointpath).start(delta_stream_table_path)
   print("Streaming to delta sink...")
    ```

Dieser Code schreibt die Streamingger√§tedaten im Deltaformat in einen Ordner mit dem Namen iotdevicedata. Da der Pfad f√ºr den Ordnerspeicherort im Ordner Tabellen angegeben ist, wird automatisch eine Tabelle f√ºr ihn erstellt.

1. F√ºgen Sie den folgenden Code in einer neuen Codezelle hinzu, und f√ºhren Sie ihn aus:

    ```python
   %%sql
   SELECT * FROM IotDeviceData;
    ```

Dieser Code fragt die Tabelle IotDeviceData ab, die die Ger√§tedaten aus der Streamingquelle enth√§lt.

1. F√ºgen Sie den folgenden Code in einer neuen Codezelle hinzu, und f√ºhren Sie ihn aus:

    ```python
   # Add more data to the source stream
   more_data = '''{"device":"Dev1","status":"ok"}
   {"device":"Dev1","status":"ok"}
   {"device":"Dev1","status":"ok"}
   {"device":"Dev1","status":"ok"}
   {"device":"Dev1","status":"error"}
   {"device":"Dev2","status":"error"}
   {"device":"Dev1","status":"ok"}'''

   mssparkutils.fs.put(inputPath + "more-data.txt", more_data, True)
    ```

Dieser Code schreibt weitere hypothetische Ger√§tedaten in die Streamingquelle.

1. F√ºhren Sie die Zelle mit dem folgenden Code erneut aus:

    ```python
   %%sql
   SELECT * FROM IotDeviceData;
    ```

Dieser Code fragt die IotDeviceData-Tabelle erneut ab, die nun die zus√§tzlichen Daten enthalten sollte, die der Streamingquelle hinzugef√ºgt wurden.

1. F√ºgen Sie in einer neuen Codezelle Code hinzu, um den Datenstrom zu beenden und die Zelle auszuf√ºhren:

    ```python
   deltastream.stop()
    ```

## Bereinigen von Ressourcen

In dieser √úbung haben Sie gelernt, wie man mit Deltatabellen in Microsoft Fabric arbeitet.

Wenn Sie mit der Erkundung Ihres Lakehouses fertig sind, k√∂nnen Sie den f√ºr diese √úbung erstellten Arbeitsbereich l√∂schen.

1. W√§hlen Sie auf der Leiste auf der linken Seite das Symbol f√ºr Ihren Arbeitsbereich aus, um alle darin enthaltenen Elemente anzuzeigen.
1. Geben Sie Feld Men√º auf der Symbolleiste, w√§hlen Sie **Arbeitsbereichseinstellungen**.
1. W√§hlen Sie im Abschnitt Allgemein die Option **Diesen Arbeitsbereich entfernen**.
