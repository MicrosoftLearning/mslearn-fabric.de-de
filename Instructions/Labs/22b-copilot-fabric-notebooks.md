---
lab:
  title: Analysieren von Daten mit Apache Spark und Copilot in Microsoft Fabric-Notebooks
  module: Get started with Copilot in Fabric for data engineering
---

# Analysieren von Daten mit Apache Spark und Copilot in Microsoft Fabric-Notebooks

In diesem Lab verwenden wir Copilot f√ºr Fabric Data Engineering, um Daten in einem Lakehouse mithilfe eines Notebooks zu laden, zu transformieren und zu speichern. Notebooks bieten eine interaktive Umgebung, die Code, Visualisierungen und narrativen Text in einem einzigen Dokument kombiniert. Dieses Format erleichtert es Ihnen, Ihren Workflow zu dokumentieren, Ihre Begr√ºndung zu erl√§utern und Ergebnisse mit anderen zu teilen. Mithilfe von Notebooks k√∂nnen Sie Code iterativ entwickeln und testen, Daten bei jedem Schritt visualisieren und einen klaren √úberblick √ºber Ihren Analyseprozess behalten. Dieser Ansatz verbessert die Zusammenarbeit, Reproduzierbarkeit und das Verst√§ndnis, was Notebooks zu einem idealen Tool f√ºr Datentechnik- und -analyseaufgaben macht.

In der Regel m√ºssen Sie beim Arbeiten mit Notebooks f√ºr Datentechnik Code in Sprachen wie Python oder Scala schreiben und √ºber ein solides Verst√§ndnis von Frameworks und Bibliotheken wie Apache Spark und Pandas verf√ºgen. Dies kann f√ºr diejenigen, die noch nicht mit dem Programmieren oder diesen Tools vertraut sind, eine Herausforderung darstellen. Mit Copilot in Fabric-Notebooks k√∂nnen Sie Ihre Datenaufgaben in nat√ºrlicher Sprache beschreiben, und Copilot generiert dann den erforderlichen Code f√ºr Sie. Dadurch wird ein Gro√üteil der technisch komplexen Aufgaben √ºbernommen, und Sie k√∂nnen sich auf Ihre Analyse konzentrieren.

Diese √úbung dauert ca. **30** Minuten.

## Lerninhalte

Nach Abschluss dieses Labs k√∂nnen Sie Folgendes:

- Erstellen und Konfigurieren eines Microsoft Fabric-Arbeitsbereichs und eines Lakehouse f√ºr Datentechnikaufgaben
- Verwenden von Copilot in Fabric-Notebooks, um Code aus Prompts in nat√ºrlicher Sprache zu generieren
- Erfassen, Bereinigen und Transformieren von Daten mit Apache Spark und Copilot-gest√ºtzten Workflows
- Normalisieren und Aufbereiten statistischer Datasets f√ºr die Analyse, indem Sie Datentypen aufteilen, filtern und konvertieren
- Speichern von transformierten Daten als Tabelle im Lakehouse f√ºr Downstreamanalysen
- Verwenden von Copilot, um Abfragen und Visualisierungen f√ºr das Durchsuchen und Validieren von Daten zu generieren
- Bew√§hrte Methoden f√ºr die Datenbereinigung, -transformation und zusammenarbeitsorientierte Analyse in Microsoft Fabric

## Vor der Installation

Sie ben√∂tigen eine [Microsoft Fabric-Kapazit√§t (F2 oder h√∂her)](https://learn.microsoft.com/fabric/fundamentals/copilot-enable-fabric) mit aktiviertem Copilot, um diese √úbung abzuschlie√üen.

> **Hinweis:** Zur Vereinfachung steht Ihnen ein Notebook mit allen Prompts f√ºr diese √úbung zum Download zur Verf√ºgung:

`https://github.com/MicrosoftLearning/mslearn-fabric/raw/refs/heads/main/Allfiles/Labs/22b/Starter/eurostat-notebook.ipynb`

## √úbungsszenario

Stellen wir uns vor, dass Contoso Health, ein Krankenhausnetzwerk mit mehreren Fachbereichen, seine Dienste in der EU erweitern und die projizierten Bev√∂lkerungsdaten analysieren m√∂chte. In diesem Beispiel wird das Dataset f√ºr die Bev√∂lkerungsprojektion von [Eurostat](https://ec.europa.eu/eurostat/web/main/home) (statistisches Amt der Europ√§ischen Union) verwendet.

Quelle: EUROPOP2023, Bev√∂lkerung am 1. Januar nach Alter, Geschlecht und Projektionstyp [[proj_23np](https://ec.europa.eu/eurostat/databrowser/product/view/proj_23np?category=proj.proj_23n)], zuletzt aktualisiert am 28.¬†Juni 2023.

## Erstellen eines Arbeitsbereichs

Erstellen Sie vor dem Arbeiten mit Daten in Fabric einen Arbeitsbereich mit aktivierter Fabric-Version. Ein Arbeitsbereich in Microsoft Fabric dient als Umgebung f√ºr die Zusammenarbeit, in der Sie alle Ihre Datentechnikartefakte organisieren und verwalten k√∂nnen, einschlie√ülich Lakehouses, Notebooks und Datasets. Stellen Sie sich diesen als Projektordner vor, der alle Ressourcen enth√§lt, die f√ºr Ihre Datenanalyse ben√∂tigt werden.

1. Navigieren Sie in einem Browser unter `https://app.fabric.microsoft.com/home?experience=fabric` zur [Microsoft Fabric-Startseite](https://app.fabric.microsoft.com/home?experience=fabric)¬†und melden Sie sich mit Ihren Fabric-Anmeldeinformationen an.

1. W√§hlen Sie auf der Men√ºleiste auf der linken Seite **Arbeitsbereiche** aus (Symbol √§hnelt &#128455;).

1. Erstellen Sie einen neuen Arbeitsbereich mit einem Namen Ihrer Wahl, und w√§hlen Sie einen Lizenzierungsmodus mit Fabric-Kapazit√§ten aus (*Premium* oder *Fabric*). Beachten Sie, dass die *Testversion* nicht unterst√ºtzt wird.
   
    > **Warum ist das wichtig?** Copilot ben√∂tigt eine kostenpflichtige Fabric-Kapazit√§t, um zu funktionieren. Dadurch wird sichergestellt, dass Sie Zugriff auf die KI-gest√ºtzten Features haben, die beim Generieren von Code in diesem Lab helfen.

1. Wenn Ihr neuer Arbeitsbereich ge√∂ffnet wird, sollte er leer sein.

    ![Screenshot eines leeren Arbeitsbereichs in Fabric](./Images/new-workspace.png)

## Erstellen eines Lakehouse

Da Sie nun einen Arbeitsbereich besitzen, ist es an der Zeit, ein Lakehouse zu erstellen, in dem Daten erfasst werden sollen. Ein Lakehouse kombiniert die Vorteile eines Data Lake (Speichern von Rohdaten in verschiedenen Formaten) mit einem Data Warehouse (strukturierte Daten, die f√ºr Analysen optimiert sind). Es dient sowohl als Speicherort f√ºr unsere Rohdaten zur Bev√∂lkerung als auch als Ziel f√ºr unser bereinigtes, transformiertes Dataset.

1. W√§hlen Sie in der Men√ºleiste auf der linken Seite **Erstellen** aus. W√§hlen Sie auf der Seite *Neu* unter dem Abschnitt *Datentechnik* die Option **Lakehouse** aus. W√§hlen Sie einen eindeutigen Namen Ihrer Wahl aus.

    >**Hinweis**: Wenn die Option **Erstellen** nicht an die Seitenleiste angeheftet ist, m√ºssen Sie zuerst die Ellipses-Option (**‚Ä¶**) ausw√§hlen.

![Screenshot der Schaltfl√§che ‚ÄûErstellen‚Äú in Fabric](./Images/copilot-fabric-notebook-create.png)

Nach etwa einer Minute wird ein neues leeres Lakehouse erstellt.

![Screenshot: Neues Lakehouse](./Images/new-lakehouse.png)

## Erstellen eines Notebooks

Sie k√∂nnen nun ein Fabric-Notizbuch erstellen, um mit Ihren Daten zu arbeiten. Notebooks bieten eine interaktive Umgebung, in der Sie Code schreiben und ausf√ºhren, Ergebnisse visualisieren und Ihren Datenanalyseprozess dokumentieren k√∂nnen. Sie eignen sich ideal f√ºr explorative Datenanalysen und iterative Entwicklung, da Sie die Ergebnisse der einzelnen Schritte sofort sehen k√∂nnen.

1. W√§hlen Sie in der Men√ºleiste auf der linken Seite **Erstellen** aus. W√§hlen Sie auf der Seite *Neu* im Abschnitt *Datentechnik* die Option **Notebook** aus.

    Ein neues Notebook mit dem Namen **Notebook 1** wird erstellt und ge√∂ffnet.

    ![Screenshot eines neuen Notebooks](./Images/new-notebook.png)

1. Fabric weist jedem Notizbuch, das Sie erstellen, einen Namen zu, z. B. Notizbuch 1, Notizbuch 2 usw. Klicken Sie auf das Bedienfeld oberhalb der Registerkarte **Home** im Men√º, um den Namen in einen aussagekr√§ftigeren Namen zu √§ndern.

    ![Screenshot eines neuen Notebooks mit der M√∂glichkeit zum Umbenennen](./Images/copilot-fabric-notebook-rename.png)

1. Markieren Sie die erste Zelle (die momentan eine Codezelle ist), und verwenden Sie dann in der oberen rechten Symbolleiste die Schaltfl√§che **M‚Üì**, um sie in eine Abschriftenzelle umzuwandeln. Der in der Zelle enthaltene Text wird dann als formatierter Text angezeigt.

    > **Gr√ºnde f√ºr die Verwendung von Markdownzellen**: Markdownzellen erm√∂glichen es Ihnen, Ihre Analyse mit formatiertem Text zu dokumentieren, sodass Ihr Notebook f√ºr andere (oder Sie selbst, wenn Sie sp√§ter darauf zur√ºckkehren) besser lesbar und verst√§ndlich wird.

    ![Screenshot eines Notebooks, in dem die erste Zelle in Markdown ge√§ndert wird](./Images/copilot-fabric-notebook-markdown.png)

1. Verwenden Sie die Taste üñâ (Bearbeiten), um die Zelle in den Bearbeitungsmodus zu schalten, und √§ndern Sie dann den Markdown wie unten gezeigt.

    ```md
    # Explore Eurostat population data.
    Use this notebook to explore population data from Eurostat
    ```
    
    ![Bildschirmfoto eines Fabric-Notebooks mit einer Markdownzelle.](Images/copilot-fabric-notebook-step-1-created.png)
    
    Wenn Sie fertig sind, klicken Sie auf eine beliebige Stelle im Notebook au√üerhalb der Zelle, um die Bearbeitung zu beenden.

## Anf√ºgen des Lakehouse an Ihr Notebook

Um mit Daten in Ihrem Lakehouse aus dem Notebook zu arbeiten, m√ºssen Sie das Lakehouse an Ihr Notebook anf√ºgen. Diese Verbindung erm√∂glicht es Ihrem Notebook, aus dem Lakehousespeicher zu lesen und in diesen zu schreiben, wodurch eine nahtlose Integration zwischen Ihrer Analyseumgebung und Ihrem Datenspeicher entsteht.

1. W√§hlen Sie Ihren neuen Arbeitsbereich in der linken Leiste aus. Sie sehen eine Liste der Elemente, die im Arbeitsbereich enthalten sind, einschlie√ülich Ihres Lakehouses und Ihres Notebooks.

1. W√§hlen Sie das Lakehouse aus, um den Explorer-Bereich anzuzeigen.

1. W√§hlen Sie im oberen Men√º **Notebook √∂ffnen** und **Vorhandenes Notebook** aus, und √∂ffnen Sie dann das Notebook, das Sie zuvor erstellt haben. Das Notebook sollte nun neben dem Explorer-Bereich ge√∂ffnet sein. Erweitern Sie die Lakehouses und dann die Liste ‚ÄûDateien‚Äú. Beachten Sie, dass neben dem Notebook-Editor noch keine Tabelle oder Dateien aufgef√ºhrt sind:

    ![Screenshot von CSV-Dateien in der Explorer-Ansicht.](Images/copilot-fabric-notebook-step-2-lakehouse-attached.png)

    > **Was zu sehen ist**: Im Explorer-Bereich auf der linken Seite wird ihre Lakehousestruktur angezeigt. Derzeit ist sie leer, aber w√§hrend wir Daten laden und verarbeiten, erscheinen Dateien im Abschnitt **Dateien** und Tabellen im Abschnitt **Tabellen**.


## Laden von Daten

Jetzt verwenden wir Copilot, um uns beim Herunterladen von Daten √ºber die Eurostat-API zu unterst√ºtzen. Anstatt Python-Code von Grund auf neu zu schreiben, beschreiben wir in nat√ºrlicher Sprache, was wir tun m√∂chten, und Copilot generiert dann den entsprechenden Code. Dies veranschaulicht einen der wichtigsten Vorteile der KI-gest√ºtzten Programmierung: Sie k√∂nnen sich auf die Gesch√§ftslogik statt auf die Details der technischen Implementierung konzentrieren.

1. Erstellen Sie eine neue Zelle in Ihrem Notebook, und kopieren Sie die folgende Anweisung in die Zelle. Um anzugeben, dass Copilot Code generieren soll, verwenden Sie `%%code` als erste Anweisung in der Zelle. 

    > **Informationen zum Magic-Befehl `%%code`**: Diese spezielle Anweisung teilt Copilot mit, dass Python-Code basierend auf Ihrer Beschreibung in nat√ºrlicher Sprache generiert werden soll. Es ist einer von mehreren ‚ÄûMagic-Befehlen‚Äú, mit denen Sie effektiver mit Copilot interagieren k√∂nnen.

    ```copilot-prompt
    %%code
    
    Download the following file from this URL:
    
    https://ec.europa.eu/eurostat/api/dissemination/sdmx/2.1/data/proj_23np$defaultview/?format=TSV
     
    Then write the file to the default lakehouse into a folder named temp. Create the folder if it doesn't exist yet.
    ```
    
1. W√§hlen Sie ‚ñ∑ **Zelle ausf√ºhren** links neben der Zelle aus, um den Code auszuf√ºhren.

    Copilot generiert den folgenden Code, der sich je nach Umgebung und den neuesten Updates f√ºr Copilot geringf√ºgig unterscheiden kann.
    
    ![Screenshot des von Copilot generierten Codes.](Images/copilot-fabric-notebook-step-3-code-magic.png)
    
    > **Funktionsweise von Copilot**: Beachten Sie, wie Copilot Ihre Anforderung in nat√ºrlicher Sprache in funktionierenden Python-Code umsetzt. Es versteht, dass Sie eine HTTP-Anforderung senden, das Dateisystem verarbeiten und die Daten an einem bestimmten Speicherort in Ihrem Lakehouse speichern m√ºssen.
    
    Im Folgenden finden Sie den vollst√§ndigen Code, falls w√§hrend der Ausf√ºhrung Ausnahmen auftreten:
    
    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    import requests
    import os
    
    # Define the URL and the local path
    url = "https://ec.europa.eu/eurostat/api/dissemination/sdmx/2.1/data/proj_23np$defaultview/?format=TSV"
    local_path = "/lakehouse/default/Files/temp/"
    file_name = "proj_23np.tsv"
    file_path = os.path.join(local_path, file_name)
    
    # Create the temporary directory if it doesn't exist
    if not os.path.exists(local_path):
        os.makedirs(local_path)
    
    # Download the file
    response = requests.get(url)
    response.raise_for_status()  # Check that the request was successful
    
    # Write the content to the file
    with open(file_path, "wb") as file:
        file.write(response.content)
    
    print(f"File downloaded and saved to {file_path}")
    ```

1. W√§hlen Sie die Option ‚ñ∑ **Zelle ausf√ºhren** links neben der Zelle aus, um den Code auszuf√ºhren und die Ausgabe zu beobachten. Die Datei sollte heruntergeladen und im tempor√§ren Ordner Ihres Lakehouse gespeichert werden.

    > **Hinweis**: M√∂glicherweise m√ºssen Sie Ihre Lakehouse-Dateien aktualisieren, indem Sie die drei Punkte ausw√§hlen.
    
    ![Screenshot der tempor√§ren Datei, die im Lakehouse erstellt wurde](Images/copilot-fabric-notebook-step-4-lakehouse-refreshed.png)

1. Nachdem sich die Rohdatendatei nun in unserem Lakehouse befindet, m√ºssen wir sie in einen Spark-DataFrame laden, damit wir sie analysieren und transformieren k√∂nnen. Erstellen Sie eine neue Zelle in Ihrem Notebook, und kopieren Sie die folgende Anweisung in die Zelle.

    > **Information** (Informationen): Ein DataFrame ist eine verteilte Datensammlung, die in benannten Spalten organisiert ist, √§hnlich wie eine Tabelle in einer Datenbank oder Kalkulationstabelle.

    ```copilot-prompt
    %%code
    
    Load the file 'Files/temp/proj_23np.tsv' into a spark dataframe.
    
    The fields are separated with a tab.
    
    Show the contents of the DataFrame using display method.
    ```

1. W√§hlen Sie die Option ‚ñ∑ **Zelle ausf√ºhren** links neben der Zelle aus, um den Code auszuf√ºhren und die Ausgabe zu beobachten. Der DataFrame sollte die Daten aus der TSV-Datei enthalten. Hier ist ein Beispiel daf√ºr, wie der generierte Code aussehen k√∂nnte:

    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    # Load the file 'Files/temp/proj_23np.tsv' into a spark dataframe.
    # The fields have been separated with a tab.
    file_path = "Files/temp/proj_23np.tsv"
    
    spark_df = spark.read.format("csv").option("delimiter", "\t").option("header", "true").load(file_path)
    
    # Show the contents of the DataFrame using display method
    display(spark_df)
    ```

Hier sehen Sie ein Beispiel f√ºr eine m√∂gliche Ausgabe:

| freq,projection,sex,age,unit,geo\TIME_PERIOD |      2022  |      2023  |   ...  |      2100  |
| -------------------------------------------- | ---------- | ---------- | ------ | ---------- |
|                         A,BSL,F,TOTAL,PER,AT |   4553444  |   4619179  |   ...  |   4807661  |
|                         A,BSL,F,TOTAL,PER,BE |   5883978  |   5947528  |   ...  |   6331785  |
|                         A,BSL,F,TOTAL,PER,BG |   3527626  |   3605059  |   ...  |   2543673  |
|                                          ... |       ...  |       ...  |   ...  |   5081250  |
|                         A,BSL,F,TOTAL,PER,CY |    463622  |    476907  |   ...  |    504781  |

> **Grundlegendes zur Datenstruktur**: Beachten Sie, dass die erste Spalte mehrere Werte enth√§lt, die durch Kommas getrennt sind (H√§ufigkeit, Projektionstyp, Geschlecht, Alter, Einheit und geografischer Standort), w√§hrend die verbleibenden Spalten Jahre mit Bev√∂lkerungswerten darstellen. Diese Struktur ist in statistischen Datasets √ºblich, muss jedoch f√ºr eine effektive Analyse bereinigt werden.

## Transformieren von Daten: Aufteilen von Feldern

Lassen Sie uns nun fortfahren und die Daten transformieren. Wir m√ºssen sicherstellen, dass das erste Feld in separate Spalten aufgeteilt wird. Dar√ºber hinaus m√ºssen wir sicherstellen, dass wir mit den richtigen Datentypen arbeiten und Filter anwenden. 

> **Warum die Felder aufgeteilt werden m√ºssen**: Die erste Spalte enth√§lt mehrere Informationen, die miteinander verkettet sind (H√§ufigkeit, Projektionstyp, Geschlecht, Altersruppe, Einheit und geografischer Code). F√ºr eine ordnungsgem√§√üe Analyse sollte jeder dieser Aspekte sich in einer eigenen Spalte befinden. Dieser Prozess wird als ‚ÄûNormalisierung‚Äú der Datenstruktur bezeichnet.

1. Erstellen Sie eine neue Zelle in Ihrem Notebook, und kopieren Sie die folgende Anweisung in die Zelle.


    ```copilot-prompt
    %%code
    
    From the currently loaded DataFrame, split the first field 'freq,projection,sex,age,unit,geo\TIME_PERIOD' using a comma into 6 separate fields.
    ```

1. W√§hlen Sie die Option ‚ñ∑ **Zelle ausf√ºhren** links neben der Zelle aus, um den Code auszuf√ºhren und die Ausgabe zu beobachten. Hier sehen Sie ein Beispiel f√ºr eine m√∂gliche Ausgabe:

    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    from pyspark.sql.functions import split, col
    
    # Split the first field 'freq,projection,sex,age,unit,geo\TIME_PERIOD' into 6 separate fields
    spark_df = spark_df.withColumn("freq", split(col("freq,projection,sex,age,unit,geo\\TIME_PERIOD"), ",").getItem(0)) \
                       .withColumn("projection", split(col("freq,projection,sex,age,unit,geo\\TIME_PERIOD"), ",").getItem(1)) \
                       .withColumn("sex", split(col("freq,projection,sex,age,unit,geo\\TIME_PERIOD"), ",").getItem(2)) \
                       .withColumn("age", split(col("freq,projection,sex,age,unit,geo\\TIME_PERIOD"), ",").getItem(3)) \
                       .withColumn("unit", split(col("freq,projection,sex,age,unit,geo\\TIME_PERIOD"), ",").getItem(4)) \
                       .withColumn("geo", split(col("freq,projection,sex,age,unit,geo\\TIME_PERIOD"), ",").getItem(5))
    
    # Show the updated DataFrame
    display(spark_df)
    ```

1. W√§hlen Sie ‚ñ∑ **Zelle ausf√ºhren** links neben der Zelle aus, um den Code auszuf√ºhren. M√∂glicherweise m√ºssen Sie in der Tabelle nach rechts scrollen, um die Felder zu sehen, die der Tabelle hinzugef√ºgt wurden.

    ![Screenshot der resultierenden Tabelle mit zus√§tzlichen Feldern](Images/copilot-fabric-notebook-split-fields.png)

## Transformieren von Daten: Entfernen von Feldern

Einige Felder in der Tabelle haben keinen Nutzen, da sie nur einen einzelnen eindeutigen Eintrag enthalten. Gem√§√ü den bew√§hrten Methoden sollten sie aus dem Dataset entfernt werden.

> **Datenbereinigungsprinzip**: Spalten mit nur einem eindeutigen Wert bieten keinen analytischen Nutzen und k√∂nnen Ihr Dataset unn√∂tig komplex machen. Das Entfernen vereinfacht die Datenstruktur und verbessert die Leistung. In diesem Fall sind ‚Äûfreq‚Äú (H√§ufigkeit), ‚Äûage‚Äú (alle Datens√§tze zeigen TOTAL) und ‚Äûunit‚Äú (alle Datens√§tze zeigen PER f√ºr Personen) f√ºr alle Zeilen konstant.

1. Erstellen Sie eine neue Zelle in Ihrem Notebook, und kopieren Sie die folgende Anweisung in die Zelle.

    ```copilot-prompt
    %%code
    
    From the currently loaded DataFrame, remove the fields 'freq', 'age', 'unit'.
    ```

1. W√§hlen Sie die Option ‚ñ∑ **Zelle ausf√ºhren** links neben der Zelle aus, um den Code auszuf√ºhren und die Ausgabe zu beobachten. Hier sehen Sie ein Beispiel f√ºr eine m√∂gliche Ausgabe:

    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    # Remove the fields 'freq', 'age', 'unit'
    spark_df = spark_df.drop("freq", "age", "unit")
    
    # Show the updated DataFrame
    display(spark_df)
    ```

1. W√§hlen Sie ‚ñ∑ **Zelle ausf√ºhren** links neben der Zelle aus, um den Code auszuf√ºhren.

## Transformieren von Daten: Neuanordnen von Feldern

Wenn Sie Ihre Daten zun√§chst mit den wichtigsten identifizierenden Spalten organisieren, sind sie leichter lesbar und verst√§ndlich. Bei der Datenanalyse ist es √ºblich, kategorische/dimensionale Spalten (z.¬†B. Projektionstyp, Geschlecht und geografischer Standort) vor den numerischen/Measurespalten (die Bev√∂lkerungswerte nach Jahr) anzuordnen.

1. Erstellen Sie eine neue Zelle in Ihrem Notebook, und kopieren Sie die folgende Anweisung in die Zelle.

    ```copilot-prompt
    %%code
    
    From the currently loaded DataFrame, the fields 'projection', 'sex', 'geo' should be positioned first.
    ```

1. W√§hlen Sie die Option ‚ñ∑ **Zelle ausf√ºhren** links neben der Zelle aus, um den Code auszuf√ºhren und die Ausgabe zu beobachten. Hier sehen Sie ein Beispiel f√ºr eine m√∂gliche Ausgabe:

    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    # Reorder the DataFrame with 'projection', 'sex', 'geo' fields first
    new_column_order = ['projection', 'sex', 'geo'] + [col for col in spark_df.columns if col not in {'projection', 'sex', 'geo'}]
    spark_df = spark_df.select(new_column_order)
    
    # Show the reordered DataFrame
    display(spark_df)
    ```

1. W√§hlen Sie ‚ñ∑ **Zelle ausf√ºhren** links neben der Zelle aus, um den Code auszuf√ºhren.

## Transformieren von Daten: Ersetzen von Werten

Das Projektionsfeld enth√§lt derzeit kryptische Codes, die nicht benutzerfreundlich sind. Zur besseren Lesbarkeit und Analyse ersetzen wir diese Codes durch aussagekr√§ftige Namen, die deutlich machen, was jedes Projektionsszenario darstellt.

> **Grundlegendes zu Projektionsszenarios**: Statistische Organisationen verwenden h√§ufig verschiedene Szenarios, um zuk√ºnftige Ver√§nderungen der Bev√∂lkerung zu modellieren. Die Baseline stellt das wahrscheinlichste Szenario dar, w√§hrend Sensitivit√§tstests zeigen, wie sich die Bev√∂lkerung unter unterschiedlichen Annahmen zu Fruchtbarkeitsraten, Sterblichkeitsraten und Migrationsmustern √§ndern kann.

1. Erstellen Sie eine neue Zelle in Ihrem Notebook, und kopieren Sie die folgende Anweisung in die Zelle.


    ```copilot-prompt
    %%code
    
    The 'projection' field contains codes that should be replaced with the following values:
        _'BSL' -> 'Baseline projections'.
        _'LFRT' -> 'Sensitivity test: lower fertility'.
        _'LMRT' -> 'Sensitivity test: lower mortality'.
        _'HMIGR' -> 'Sensitivity test: higher migration'.
        _'LMIGR' -> 'Sensitivity test: lower migration'.
        _'NMIGR' -> 'Sensitivity test: no migration'.
    ```

1. W√§hlen Sie die Option ‚ñ∑ **Zelle ausf√ºhren** links neben der Zelle aus, um den Code auszuf√ºhren und die Ausgabe zu beobachten. Hier sehen Sie ein Beispiel f√ºr eine m√∂gliche Ausgabe:

    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    from pyspark.sql.functions import when
    
    # Replace projection codes
    spark_df = spark_df.withColumn("projection", 
                                   when(spark_df["projection"] == "BSL", "Baseline projections")
                                   .when(spark_df["projection"] == "LFRT", "Sensitivity test: lower fertility")
                                   .when(spark_df["projection"] == "LMRT", "Sensitivity test: lower mortality")
                                   .when(spark_df["projection"] == "HMIGR", "Sensitivity test: higher migration")
                                   .when(spark_df["projection"] == "LMIGR", "Sensitivity test: lower migration")
                                   .when(spark_df["projection"] == "NMIGR", "Sensitivity test: no migration")
                                   .otherwise(spark_df["projection"]))
    
    # Display the updated DataFrame
    display(spark_df)
    ```

1. W√§hlen Sie ‚ñ∑ **Zelle ausf√ºhren** links neben der Zelle aus, um den Code auszuf√ºhren.

    ![Screenshot der resultierenden Tabelle mit ersetzten Projektfeldwerten](Images/copilot-fabric-notebook-replace-values.png)
    
## Transformieren von Daten: Filtern von Daten

Die Tabelle mit den Bev√∂lkerungsprojektionen enth√§lt zwei Zeilen f√ºr L√§nder, die es als solche nicht gibt: EU27_2020 (*Gesamtsummen f√ºr die Europ√§ische Union ‚Äì 27 L√§nder*) und EA20 (*Eurozone ‚Äì 20 L√§nder*). Wir m√ºssen diese beiden Zeilen entfernen, da die Daten nur das niedrigste Aggregationsintervall haben sollen.

> **Datengranularit√§tsprinzip**: F√ºr eine detaillierte Analyse ist es wichtig, mit Daten auf m√∂glichst pr√§ziser Ebene zu arbeiten. Aggregierte Werte (z.¬†B. EU-Gesamtwerte) k√∂nnen bei Bedarf immer berechnet werden, aber die Aufnahme in Ihr Basisdataset kann zu doppelter Z√§hlung oder Verwirrung bei der Analyse f√ºhren.

![Screenshot der Tabelle mit hervorgehobenen geografischen R√§umen EA20 und EU2_2020.](Images/copilot-fabric-notebook-europe.png)

1. Erstellen Sie eine neue Zelle in Ihrem Notebook, und kopieren Sie die folgende Anweisung in die Zelle.

    ```copilot-prompt
    %%code
    
    Filter the 'geo' field and remove values 'EA20' and 'EU27_2020' (these are not countries).
    ```

1. W√§hlen Sie die Option ‚ñ∑ **Zelle ausf√ºhren** links neben der Zelle aus, um den Code auszuf√ºhren und die Ausgabe zu beobachten. Hier sehen Sie ein Beispiel f√ºr eine m√∂gliche Ausgabe:

    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    # Filter out 'geo' values 'EA20' and 'EU27_2020'
    spark_df = spark_df.filter((spark_df['geo'] != 'EA20') & (spark_df['geo'] != 'EU27_2020'))
    
    # Display the filtered DataFrame
    display(spark_df)
    ```

1. W√§hlen Sie ‚ñ∑ **Zelle ausf√ºhren** links neben der Zelle aus, um den Code auszuf√ºhren.

    Die Tabelle f√ºr die Bev√∂lkerungsprojektion beinhaltet auch das Feld ‚Äûsex‚Äú, das die folgenden eindeutigen Werte enth√§lt:
    
    - M: Male
    - F: Weiblich
    - T: Total (Summe von m√§nnlich + weiblich)

    Auch hier m√ºssen wir die Summen entfernen, um die Daten auf der niedrigsten Detailebene zu halten.

    > **Gr√ºnde f√ºr das Entfernen von Summen**: √Ñhnlich wie bei den geografischen Aggregationen sollen nur die einzelnen Geschlechtskategorien (m√§nnlich und weiblich) beibehalten und die Gesamtwerte ausgeschlossen werden. Das erm√∂glicht eine flexiblere Analyse. Sie k√∂nnen die Werte von ‚ÄûMale‚Äú und ‚ÄûFemale‚Äú jederzeit addieren, um Summen zu erhalten, aber Sie k√∂nnen die Summen nicht wieder in die einzelnen Summanden aufteilen.

1. Erstellen Sie eine neue Zelle in Ihrem Notebook, und kopieren Sie die folgende Anweisung in die Zelle.

    ```copilot-prompt
    %%code
    
    Filter the 'sex' field and remove 'T' (these are totals).
    ```

1. W√§hlen Sie die Option ‚ñ∑ **Zelle ausf√ºhren** links neben der Zelle aus, um den Code auszuf√ºhren und die Ausgabe zu beobachten. Hier sehen Sie ein Beispiel f√ºr eine m√∂gliche Ausgabe:

    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    # Filter out 'sex' values 'T'
    spark_df = spark_df.filter(spark_df['sex'] != 'T')
    
    # Display the filtered DataFrame
    display(spark_df)
    ```

1. W√§hlen Sie ‚ñ∑ **Zelle ausf√ºhren** links neben der Zelle aus, um den Code auszuf√ºhren.

## Transformieren von Daten: K√ºrzen von Leerzeichen

Einige Feldnamen in der Tabelle f√ºr die Bev√∂lkerungsprojektion haben ein Leerzeichen am Ende. Wir m√ºssen einen K√ºrzungsvorgang auf die Namen dieser Felder anwenden.

> **Auswirkungen auf die Datenqualit√§t**: Zus√§tzliche Leerzeichen in Spaltennamen k√∂nnen Probleme verursachen, wenn Daten abgefragt oder Visualisierungen erstellt werden. Dies ist ein h√§ufiges Problem mit der Datenqualit√§t, insbesondere wenn Daten aus externen Quellen stammen oder aus anderen Systemen exportiert werden. Das K√ºrzen von Leerzeichen sorgt f√ºr Konsistenz und verhindert Probleme, die sp√§ter schwer zu debuggen sind.

1. Erstellen Sie eine neue Zelle in Ihrem Notebook, und kopieren Sie die folgende Anweisung in die Zelle.

    ```copilot-prompt
    %%code
    
    Strip spaces from all field names in the dataframe.
    ```

1. W√§hlen Sie die Option ‚ñ∑ **Zelle ausf√ºhren** links neben der Zelle aus, um den Code auszuf√ºhren und die Ausgabe zu beobachten. Hier sehen Sie ein Beispiel f√ºr eine m√∂gliche Ausgabe:

    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    from pyspark.sql.functions import col
    
    # Strip spaces from all field names
    spark_df = spark_df.select([col(column).alias(column.strip()) for column in spark_df.columns])
    
    # Display the updated DataFrame
    display(spark_df)
    ```

1. W√§hlen Sie ‚ñ∑ **Zelle ausf√ºhren** links neben der Zelle aus, um den Code auszuf√ºhren.

## Transformieren von Daten: Datentypkonvertierung

Wenn wir die Daten sp√§ter ordnungsgem√§√ü analysieren m√∂chten (z.¬†B. mit Power BI oder SQL), m√ºssen wir sicherstellen, dass die Datentypen (z.¬†B. Zahlen und Datum/Uhrzeit) richtig festgelegt sind. 

> **Wichtigkeit der richtigen Datentypen**: Wenn Daten aus Textdateien geladen werden, werden zun√§chst alle Spalten als Zeichenfolgen behandelt. Das Konvertieren von Jahresspalten in ganze Zahlen erm√∂glicht mathematische Operationen (z.¬†B. Berechnungen und Aggregationen) und eine ordnungsgem√§√üe Sortierung. Dieser Schritt ist f√ºr Downstreamanalyse- und Visualisierungstools entscheidend.

1. Erstellen Sie eine neue Zelle in Ihrem Notebook, und kopieren Sie die folgende Anweisung in die Zelle.

    ```copilot-prompt
    %%code
    
    Convert the data type of all the year fields to integer.
    ```

1. W√§hlen Sie die Option ‚ñ∑ **Zelle ausf√ºhren** links neben der Zelle aus, um den Code auszuf√ºhren und die Ausgabe zu beobachten. Hier sehen Sie ein Beispiel f√ºr eine m√∂gliche Ausgabe:

    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    from pyspark.sql.functions import col
    
    # Convert the data type of all the year fields to integer
    year_columns = [col(column).cast("int") for column in spark_df.columns if column.strip().isdigit()]
    spark_df = spark_df.select(*spark_df.columns[:3], *year_columns)
    
    # Display the updated DataFrame
    display(spark_df)
    ```
    
1. W√§hlen Sie ‚ñ∑ **Zelle ausf√ºhren** links neben der Zelle aus, um den Code auszuf√ºhren. Hier ist ein Beispiel daf√ºr, wie die Ausgabe aussehen k√∂nnte (Spalten und Zeilen aus Platzgr√ºnden entfernt):

|          Projektion (projection)|sex|geo|    2022|    2023|     ...|    2100|
|--------------------|---|---|--------|--------|--------|--------| 
|Baseline projections|  F| AT| 4553444| 4619179|     ...| 4807661|
|Baseline projections|  F| BE| 5883978| 5947528|     ...| 6331785|
|Baseline projections|  F| BG| 3527626| 3605059|     ...| 2543673|
|...                 |...|...|     ...|     ...|     ...|     ...|
|Baseline projections|  F| LU|  320333|  329401|     ...|  498954|

>[!TIP]
> M√∂glicherweise m√ºssen Sie in der Tabelle nach rechts scrollen, um alle Spalten zu sehen.

## Speichern von Daten

Als N√§chstes speichern wir die transformierten Daten in unserem Lakehouse. 

> **Gr√ºnde f√ºr das Speichern der transformierten Daten**: Nach dem Aufwand f√ºr die Datenbereinigung und -transformation sollen die Ergebnisse beibehalten werden. Das Speichern der Daten als Tabelle im Lakehouse erm√∂glicht es uns und anderen, dieses bereinigte Dataset f√ºr verschiedene Analyseszenarios zu verwenden, ohne den Transformationsprozess wiederholen zu m√ºssen. Au√üerdem k√∂nnen andere Tools im Microsoft Fabric-√ñkosystem (z.¬†B. Power BI, SQL-Analyse-Endpunkte und Data Factory) mit diesen Daten arbeiten.

1. Erstellen Sie eine neue Zelle in Ihrem Notebook, und kopieren Sie die folgende Anweisung in die Zelle.

    ```copilot-prompt
    %%code
    
    Save the dataframe as a new table named 'Population' in the default lakehouse.
    ```
    
1. W√§hlen Sie ‚ñ∑ **Zelle ausf√ºhren** links neben der Zelle aus, um den Code auszuf√ºhren. Copilot generiert den Code, der sich je nach Umgebung und den neuesten Updates f√ºr Copilot geringf√ºgig unterscheiden kann.

    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    spark_df.write.format("delta").saveAsTable("Population")
    ```

1. W√§hlen Sie ‚ñ∑ **Zelle ausf√ºhren** links neben der Zelle aus, um den Code auszuf√ºhren.

## Validierung durch Fragen

Testen wir nun die Leistung von Copilot f√ºr die Datenanalyse. Anstatt komplexe SQL-Abfragen oder Visualisierungscode von Grund auf neu zu schreiben, k√∂nnen wir Copilot Fragen in nat√ºrlicher Sprache zu unseren Daten stellen, und es wird der entsprechende Code generiert, um sie zu beantworten.

1. Um zu best√§tigen, dass die Daten ordnungsgem√§√ü gespeichert sind, erweitern Sie die Tabellen in Ihrem Lakehouse, und √ºberpr√ºfen Sie den Inhalt. M√∂glicherweise m√ºssen Sie den Ordner ‚ÄûTabellen‚Äú aktualisieren, indem Sie die drei Punkte ausw√§hlen. 

    ![Screenshot: Lakehouse mit einer neuen Tabelle namens ‚ÄûPopulation‚Äú](Images/copilot-fabric-notebook-step-5-lakehouse-refreshed.png)

1. W√§hlen Sie im Men√ºband ‚ÄûStart‚Äú die Option ‚ÄûCopilot‚Äú aus.

    > **Copilot-Chatoberfl√§che**: Das Copilot-Panel enth√§lt eine Chatoberfl√§che, in der Sie Fragen zu Ihren Daten in nat√ºrlicher Sprache stellen k√∂nnen. Copilot kann Code f√ºr die Analyse generieren, Visualisierungen erstellen und Ihnen helfen, Muster in Ihrem Dataset zu untersuchen.

    ![Screenshot des Notebooks mit ge√∂ffnetem Copilot-Panel](Images/copilot-fabric-notebook-step-6-copilot-pane.png)

1. Geben Sie den folgenden Prompt ein:

    ```copilot-prompt
    What are the projected population trends for geo BE  from 2020 to 2050 as a line chart visualization. Make sure to sum up male and female numbers. Use only existing columns from the population table. Perform the query using SQL.
    ```

    > **Was dieses Beispiel veranschaulicht**: Dieser Prompt veranschaulicht die F√§higkeit von Copilot, den Kontext (unsere Population-Tabelle) zu verstehen, SQL-Abfragen zu generieren und Visualisierungen zu erstellen. Das ist besonders n√ºtzlich, da Datenabfragen mit Visualisierungen in einer einzelnen Anforderung kombiniert werden.

1. Sehen Sie sich die generierte Ausgabe an, die sich je nach Umgebung und den neuesten Updates f√ºr Copilot geringf√ºgig unterscheiden kann. Kopieren Sie das Codefragment in eine neue Zelle.

    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    import plotly.graph_objs as go
    
    # Perform the SQL query to get projected population trends for geo BE, summing up male and female numbers
    result = spark.sql(
        """
        SELECT projection, sex, geo, SUM(`2022`) as `2022`, SUM(`2023`) as `2023`, SUM(`2025`) as `2025`,
               SUM(`2030`) as `2030`, SUM(`2035`) as `2035`, SUM(`2040`) as `2040`,
               SUM(`2045`) as `2045`, SUM(`2050`) as `2050`
        FROM Population
        WHERE geo = 'BE' AND projection = 'Baseline projections'
        GROUP BY projection, sex, geo
        """
    )
    df = result.groupBy("projection").sum()
    df = df.orderBy("projection").toPandas()
    
    # Extract data for the line chart
    years = df.columns[1:].tolist()
    values = df.iloc[0, 1:].tolist()
    
    # Create the plot
    fig = go.Figure()
    fig.add_trace(go.Scatter(x=years, y=values, mode='lines+markers', name='Projected Population'))
    
    # Update the layout
    fig.update_layout(
        title='Projected Population Trends for Geo BE (Belgium) from 2022 to 2050',
        xaxis_title='Year',
        yaxis_title='Population',
        template='plotly_dark'
    )
    
    # Display the plot
    fig.show()
    ```

1. W√§hlen Sie ‚ñ∑ **Zelle ausf√ºhren** links neben der Zelle aus, um den Code auszuf√ºhren. 

    Sehen Sie sich das erstellte Diagramm an:
    
    ![Screenshot des Notebooks mit erstelltem Liniendiagramm](Images/copilot-fabric-notebook-step-7-line-chart.png)
    
    > **Was Sie erreicht haben**: Sie haben Copilot erfolgreich verwendet, um eine Visualisierung zu generieren, die die Bev√∂lkerungstrends f√ºr Belgien im Laufe der Zeit anzeigt. Dadurch wird der gesamte Datentechnikworkflow veranschaulicht: Erfassung, Transformation, Speicherung und Analyse von Daten ‚Äì alles mit KI-Unterst√ºtzung.

## Bereinigen von Ressourcen

In dieser √úbung haben Sie gelernt, wie Sie Copilot und Spark verwenden, um mit Daten in Microsoft Fabric zu arbeiten.

Wenn Sie mit der Untersuchung Ihrer Daten fertig sind, k√∂nnen Sie die Spark-Sitzung beenden und den Arbeitsbereich l√∂schen, den Sie f√ºr diese √úbung erstellt haben.

1.  W√§hlen Sie im Notebookmen√º **Sitzung beenden** aus, um die Spark-Sitzung zu beenden.
1.  W√§hlen Sie auf der Leiste auf der linken Seite das Symbol f√ºr Ihren Arbeitsbereich aus, um alle darin enthaltenen Elemente anzuzeigen.
1.  W√§hlen Sie **Arbeitsbereichseinstellungen** und scrollen Sie im Abschnitt **Allgemein** nach unten und w√§hlen Sie **Diesen Arbeitsbereich entfernen**.
1.  W√§hlen Sie **L√∂schen**, um den Arbeitsbereich zu l√∂schen.
